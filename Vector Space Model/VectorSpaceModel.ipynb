{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inverted_index = {}\n",
    "N = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    tokens_wo_stopwords = []\n",
    "    for i in range(0,len(tokens)):\n",
    "        if tokens[i].lower() not in stopwords:\n",
    "            tokens_wo_stopwords.append(tokens[i].lower())\n",
    "    return tokens_wo_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pos_tag(token):\n",
    "    pos_tag = nltk.pos_tag([token])[0][1]\n",
    "    if pos_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif pos_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lemmatize(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for i in range(0,len(tokens)):\n",
    "        tokens[i] = lemmatizer.lemmatize(tokens[i],pos=str(get_pos_tag(tokens[i])))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_to_inverted_index(tokens,index):\n",
    "    for i in range(0,len(tokens)):\n",
    "        if tokens[i] not in inverted_index:\n",
    "            inverted_index[tokens[i]] = {\n",
    "                str(index): 1\n",
    "            }\n",
    "        else:\n",
    "            if str(index) not in inverted_index[tokens[i]]:\n",
    "                inverted_index[tokens[i]][str(index)] = 1\n",
    "            else:\n",
    "                inverted_index[tokens[i]][str(index)] = inverted_index[tokens[i]][str(index)] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save(inverted_index,filename):\n",
    "    with open(filename + '.pkl','wb') as index:\n",
    "        pickle.dump(inverted_index,index,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read():\n",
    "    with open(\"inverted_index.pkl\",'rb') as file:\n",
    "        inverted_index = pickle.load(file)\n",
    "    with open(\"bi_word_inverted_index.pkl\",'rb') as file1:\n",
    "        bi_word_inverted_index = pickle.load(file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess():\n",
    "    for i in range(5,100,5):\n",
    "        with open(\"data/data_split_\" + str(i) + \".csv\") as file:\n",
    "            csv_reader = csv.reader(file,delimiter=',')\n",
    "            flag = 0\n",
    "            for row in csv_reader:\n",
    "                row = re.sub(r'[^a-zA-Z]', ' ', str(row))\n",
    "                tokens = word_tokenize(str(row))\n",
    "                tokens = remove_stopwords(tokens)\n",
    "                tokens = lemmatize(tokens)\n",
    "                add_to_inverted_index(tokens,i)\n",
    "    save(inverted_index,\"inverted_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_idf(df):\n",
    "    if df != 0:\n",
    "        return math.log10(N/df)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tf(documents):\n",
    "    tf = 0\n",
    "    for key in documents.keys():\n",
    "        tf += docuemnts[key]\n",
    "    if tf != 0:\n",
    "        return (1 + math.log10(tf))\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cosine_score(query_vec,doc_vecs):\n",
    "    cosine_scores = {}\n",
    "    for key in doc_vecs.keys():\n",
    "        prod = 0\n",
    "        tf_sq = 0\n",
    "        tfd_sq = 0\n",
    "        for i in range(0,len(query_vec)):\n",
    "            prod += query_vec[i] * doc_vecs[key][i]\n",
    "            tf_sq += query_vec[i] ** 2\n",
    "            tfd_sq += doc_vecs[key][i] ** 2\n",
    "        cosine_scores[key] = prod/(tf_sq * tfd_sq)\n",
    "    return cosine_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_relevant_documents(query_tokens):\n",
    "    query_vec = []\n",
    "    doc_vecs = {}\n",
    "    for i in range(0,len(query_tokens)):\n",
    "        documents = inverted_index[query_tokens[i]]\n",
    "        idf = get_idf(len(documents.keys()))\n",
    "        tf = get_tf(documents)\n",
    "        tf_idf = tf * idf\n",
    "        query_vec.append(tf_idf)\n",
    "        for j in range(5,100,5):\n",
    "            key = str(j)\n",
    "            if key not in doc_vecs:\n",
    "                doc_vecs[key] = []\n",
    "            if key in documents:\n",
    "                doc_vecs[key].append(documents[key])\n",
    "            else:\n",
    "                doc_vecs[key].append(0)\n",
    "        consine_scores = get_cosine_scores(query_vec,doc_vecs)\n",
    "        cosine_scores = sorted(csine_scores.items(), key=lambda x: x[1],reverse=True)\n",
    "        print(\"Most relevant documents are:- \")\n",
    "        for i in range(0,min(5,len(cosine_scores))):\n",
    "            print(cosine_scores[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_and_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = input(\"Enter query:- \")\n",
    "query_tokens = query.split(' ')\n",
    "find_relevant_documents(query_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
