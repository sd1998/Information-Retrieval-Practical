{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k_gram_index = {}\n",
    "k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    tokens_wo_stopwords = []\n",
    "    for i in range(0,len(tokens)):\n",
    "        if tokens[i].lower() not in stopwords:\n",
    "            tokens_wo_stopwords.append(tokens[i].lower())\n",
    "    return tokens_wo_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save(inverted_index,filename):\n",
    "    with open(filename + '.pkl','wb') as index:\n",
    "        pickle.dump(inverted_index,index,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read():\n",
    "    with open(\"inverted_index.pkl\",'rb') as file:\n",
    "        inverted_index = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess():\n",
    "    for i in range(3,600,3):\n",
    "        with open(\"data/data_split_\" + str(i) + \".csv\") as file:\n",
    "            csv_reader = csv.reader(file,delimiter=',')\n",
    "            flag = 0\n",
    "            for row in csv_reader:\n",
    "                row = re.sub(r'[^a-zA-Z]', ' ', str(row))\n",
    "                tokens = word_tokenize(str(row))\n",
    "                tokens = remove_stopwords(tokens)\n",
    "                add_to_inverted_index(tokens,i)\n",
    "    save(k_gram_index,\"inverted_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_to_inverted_index(tokens,index):\n",
    "    for i in range(0,len(tokens)):\n",
    "        token = '$' + tokens[i] + '$'\n",
    "        for j in range(0,len(token) - k):\n",
    "            substr = token[j:j + k]\n",
    "            if substr not in k_gram_index:\n",
    "                k_gram_index[substr] = [tokens[i]]\n",
    "            else:\n",
    "                if tokens[i] not in k_gram_index[substr]:\n",
    "                    k_gram_index[substr].append(tokens[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_correct_spelling(word):\n",
    "    result = {}\n",
    "    query_word = '$' + word + '$'\n",
    "    for i in range(0,len(query_word) - k):\n",
    "        substr = query_word[i:i + k]\n",
    "        if substr in k_gram_index:\n",
    "            appeared_in = k_gram_index[substr]\n",
    "            for j in range(0,len(appeared_in)):\n",
    "                if appeared_in[j] not in result:\n",
    "                    result[appeared_in[j]] = 1\n",
    "                else:\n",
    "                    result[appeared_in[j]] += 1\n",
    "    result = sorted(result.items(), key=lambda x: x[1],reverse=True)\n",
    "    print('Correct spelling for ' + word + \":- \")\n",
    "    for i in range(0,min(len(result),5)):\n",
    "        print(result[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_and_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter query:- frends\n",
      "Correct spelling for frends:- \n",
      "friendship\n",
      "french\n",
      "friends\n",
      "frenzy\n",
      "frenemied\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Enter query:- \")\n",
    "isolated_words = query.split(' ')\n",
    "for i in range(0,len(isolated_words)):\n",
    "    get_correct_spelling(isolated_words[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
