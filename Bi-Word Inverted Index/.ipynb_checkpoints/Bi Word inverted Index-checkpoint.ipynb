{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inverted_index = {}\n",
    "bi_word_inverted_index = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    tokens_wo_stopwords = []\n",
    "    for i in range(0,len(tokens)):\n",
    "        if tokens[i].lower() not in stopwords:\n",
    "            tokens_wo_stopwords.append(tokens[i].lower())\n",
    "    return tokens_wo_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pos_tag(token):\n",
    "    pos_tag = nltk.pos_tag([token])[0][1]\n",
    "    if pos_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif pos_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lemmatize(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for i in range(0,len(tokens)):\n",
    "        tokens[i] = lemmatizer.lemmatize(tokens[i],pos=str(get_pos_tag(tokens[i])))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_to_inverted_index(tokens,index):\n",
    "    for i in range(0,len(tokens)):\n",
    "        if tokens[i] not in inverted_index:\n",
    "            inverted_index[tokens[i]] = {index: [i]}\n",
    "        else:\n",
    "            if index not in inverted_index[tokens[i]]:\n",
    "                inverted_index[tokens[i]][index] = [i]\n",
    "            else:\n",
    "                inverted_index[tokens[i]][index].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_to_bi_word_inverted_index(tokens,index):\n",
    "    for i in range(0,len(tokens)-1):\n",
    "        token = tokens[i] + \" \" + tokens[i+1]\n",
    "        if token not in bi_word_inverted_index:\n",
    "            bi_word_inverted_index[token] = {index: [i]}\n",
    "        else:\n",
    "            if index not in bi_word_inverted_index[token]:\n",
    "                bi_word_inverted_index[token][index] = [i]\n",
    "            else:\n",
    "                bi_word_inverted_index[token][index].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save(inverted_index,filename):\n",
    "    with open(filename + '.pkl','wb') as index:\n",
    "        pickle.dump(inverted_index,index,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read():\n",
    "    with open(\"inverted_index.pkl\",'rb') as file:\n",
    "        inverted_index = pickle.load(file)\n",
    "    with open(\"bi_word_inverted_index.pkl\",'rb') as file1:\n",
    "        bi_word_inverted_index = pickle.load(file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess():\n",
    "    for i in range(5,100,5):\n",
    "        with open(\"data/data_split_\" + str(i) + \".csv\") as file:\n",
    "            csv_reader = csv.reader(file,delimiter=',')\n",
    "            flag = 0\n",
    "            for row in csv_reader:\n",
    "                row = re.sub(r'[^a-zA-Z]', ' ', str(row))\n",
    "                tokens = word_tokenize(str(row))\n",
    "                tokens = remove_stopwords(tokens)\n",
    "                tokens = lemmatize(tokens)\n",
    "                add_to_inverted_index(tokens,i)\n",
    "                add_to_bi_word_inverted_index(tokens,i)\n",
    "    save(inverted_index,\"inverted_index\")\n",
    "    save(bi_word_inverted_index,\"bi_word_inverted_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def and_query(words,is_bi_word=False):\n",
    "    if is_bi_word:\n",
    "        index = bi_word_inverted_index\n",
    "    else:\n",
    "        index = inverted_index\n",
    "    first_word = True\n",
    "    result = []\n",
    "    for word in words:\n",
    "        word = word.strip()\n",
    "        if word not in index:\n",
    "            return []\n",
    "        if first_word:\n",
    "            first_word = False\n",
    "            result = list(index[word].keys())\n",
    "        else:\n",
    "            result_temp = []\n",
    "            for i in range(0,len(index[word].keys())):\n",
    "                if list(index[word].keys())[i] in result:\n",
    "                    result_temp.append(list(index[word].keys())[i])\n",
    "            result = []\n",
    "            result = result_temp\n",
    "            if(len(result) == 0):\n",
    "                return result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def or_query(words,is_bi_word=False):\n",
    "    if is_bi_word:\n",
    "        index = bi_word_inverted_index\n",
    "    else:\n",
    "        index = inverted_index\n",
    "    result = []\n",
    "    for word in words:\n",
    "        word = word.strip()\n",
    "        for i in range(0,len(index[word].keys())):\n",
    "            if list(index[word].keys())[i] not in result:\n",
    "                result.append(list(index[word].keys())[i])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#AND\n",
    "def not_query(words,is_bi_word=False):\n",
    "    if is_bi_word:\n",
    "        index = bi_word_inverted_index\n",
    "    else:\n",
    "        index = inverted_index\n",
    "    result = [5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100]\n",
    "    for word in words:\n",
    "        word = word.strip()\n",
    "        for i in range(0,len(index[word].keys())):\n",
    "            if list(index[word].keys())[i] in result:\n",
    "                result.remove(list(index[word].keys())[i])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def proximity_query(words,offset=100):\n",
    "    result = []\n",
    "    and_query_result = and_query(words)\n",
    "    for document in and_query_result:\n",
    "        for pos_i in inverted_index[words[0]][document]:\n",
    "            for pos_j in inverted_index[words[1]][document]:\n",
    "                if abs(pos_i - pos_j) < offset:\n",
    "                    if document not in result:\n",
    "                        result.append(document)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_and_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 10, 80]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proximity_query(['content washington','washington congressional'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
